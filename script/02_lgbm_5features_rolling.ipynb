{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f847beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Library Imports\n",
    "# =========================================================\n",
    "\n",
    "# ===== Basic Libraries =====\n",
    "import numpy as np               # Numerical computations\n",
    "import pandas as pd              # Data manipulation and analysis\n",
    "import re                        # Regular expressions for text processing\n",
    "import warnings                  # To manage and suppress warnings\n",
    "from datetime import date        # Working with date objects\n",
    "from tqdm import tqdm            # Progress bars for loops\n",
    "\n",
    "# ===== Visualization =====\n",
    "import matplotlib.pyplot as plt  # Core plotting library\n",
    "import seaborn as sns            # Statistical data visualization\n",
    "\n",
    "# ===== Statistics & Evaluation Metrics =====\n",
    "from scipy.stats import spearmanr             # Spearman rank correlation (for IC calculation)\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # Model evaluation metrics\n",
    "\n",
    "# ===== Feature Processing & Model Pipeline =====\n",
    "from sklearn.impute import SimpleImputer               # Handle missing values\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler  # Data normalization\n",
    "from sklearn.compose import ColumnTransformer          # Combine multiple preprocessing steps\n",
    "from sklearn.pipeline import Pipeline                  # Build modeling pipelines\n",
    "from sklearn.base import BaseEstimator, TransformerMixin  # Create custom transformers\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV  # Time-based CV and hyperparameter tuning\n",
    "\n",
    "# ===== Modeling =====\n",
    "import lightgbm as lgb              # LightGBM framework\n",
    "from lightgbm import LGBMRegressor  # Core LightGBM model class\n",
    "\n",
    "# ===== Optional Settings =====\n",
    "warnings.filterwarnings(\"ignore\")   # Suppress unnecessary warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a2be58",
   "metadata": {},
   "source": [
    "1. Data Import & Preprocessing  \n",
    "Ensure data cleaning, feature engineering, and time alignment with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8257d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Configuration =====\n",
    "file_path = \"../data/final_df.csv\"  \n",
    "try:\n",
    "    df_ori = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"*** ERROR: Cannot find {file_path} ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3cae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Dropped 12 unusal/NaN y value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>symbol</th>\n",
       "      <th>dow_0</th>\n",
       "      <th>dow_1</th>\n",
       "      <th>dow_2</th>\n",
       "      <th>dow_3</th>\n",
       "      <th>dow_4</th>\n",
       "      <th>dow_5</th>\n",
       "      <th>f_minsin</th>\n",
       "      <th>f_mincos</th>\n",
       "      <th>...</th>\n",
       "      <th>split_nonpos_flag</th>\n",
       "      <th>shares_out</th>\n",
       "      <th>log_shares_out</th>\n",
       "      <th>eps_surp_pct_final</th>\n",
       "      <th>div_amount</th>\n",
       "      <th>log_shares_out_iqr_outlier</th>\n",
       "      <th>eps_estimate_rz_8</th>\n",
       "      <th>eps_actual</th>\n",
       "      <th>lret_1m</th>\n",
       "      <th>y_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-30 12:51:00</td>\n",
       "      <td>AMAT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.220697</td>\n",
       "      <td>-0.975342</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>830897024</td>\n",
       "      <td>20.538016</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.000199</td>\n",
       "      <td>-0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-30 12:51:00</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.220697</td>\n",
       "      <td>-0.975342</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1616140032</td>\n",
       "      <td>21.203306</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>-0.000126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-30 12:51:00</td>\n",
       "      <td>AVGO</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.220697</td>\n",
       "      <td>-0.975342</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>465308000</td>\n",
       "      <td>19.958210</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.000334</td>\n",
       "      <td>0.000721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-30 12:51:00</td>\n",
       "      <td>MU</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.220697</td>\n",
       "      <td>-0.975342</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1107369984</td>\n",
       "      <td>20.825254</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>-0.000698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-30 12:51:00</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.220697</td>\n",
       "      <td>-0.975342</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2500000000</td>\n",
       "      <td>21.639557</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.000769</td>\n",
       "      <td>-0.000907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324883</th>\n",
       "      <td>2025-10-28 15:58:00</td>\n",
       "      <td>AMAT</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.861629</td>\n",
       "      <td>-0.507538</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>796642427</td>\n",
       "      <td>20.495916</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.000461</td>\n",
       "      <td>-0.000132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324884</th>\n",
       "      <td>2025-10-28 15:58:00</td>\n",
       "      <td>AMD</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.861629</td>\n",
       "      <td>-0.507538</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1633284837</td>\n",
       "      <td>21.213859</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>-0.001316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324885</th>\n",
       "      <td>2025-10-28 15:58:00</td>\n",
       "      <td>AVGO</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.861629</td>\n",
       "      <td>-0.507538</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4722365022</td>\n",
       "      <td>22.275576</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324886</th>\n",
       "      <td>2025-10-28 15:58:00</td>\n",
       "      <td>MU</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.861629</td>\n",
       "      <td>-0.507538</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1122466035</td>\n",
       "      <td>20.838794</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>-0.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324887</th>\n",
       "      <td>2025-10-28 15:58:00</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.861629</td>\n",
       "      <td>-0.507538</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>24347000000</td>\n",
       "      <td>23.915674</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>-0.000025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>324888 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime symbol  dow_0  dow_1  dow_2  dow_3  dow_4  dow_5  \\\n",
       "0      2024-04-30 12:51:00   AMAT      0      1      0      0      0      0   \n",
       "1      2024-04-30 12:51:00    AMD      0      1      0      0      0      0   \n",
       "2      2024-04-30 12:51:00   AVGO      0      1      0      0      0      0   \n",
       "3      2024-04-30 12:51:00     MU      0      1      0      0      0      0   \n",
       "4      2024-04-30 12:51:00   NVDA      0      1      0      0      0      0   \n",
       "...                    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "324883 2025-10-28 15:58:00   AMAT      0      1      0      0      0      0   \n",
       "324884 2025-10-28 15:58:00    AMD      0      1      0      0      0      0   \n",
       "324885 2025-10-28 15:58:00   AVGO      0      1      0      0      0      0   \n",
       "324886 2025-10-28 15:58:00     MU      0      1      0      0      0      0   \n",
       "324887 2025-10-28 15:58:00   NVDA      0      1      0      0      0      0   \n",
       "\n",
       "        f_minsin  f_mincos  ...  split_nonpos_flag   shares_out  \\\n",
       "0      -0.220697 -0.975342  ...                  1    830897024   \n",
       "1      -0.220697 -0.975342  ...                  1   1616140032   \n",
       "2      -0.220697 -0.975342  ...                  1    465308000   \n",
       "3      -0.220697 -0.975342  ...                  1   1107369984   \n",
       "4      -0.220697 -0.975342  ...                  1   2500000000   \n",
       "...          ...       ...  ...                ...          ...   \n",
       "324883 -0.861629 -0.507538  ...                  1    796642427   \n",
       "324884 -0.861629 -0.507538  ...                  1   1633284837   \n",
       "324885 -0.861629 -0.507538  ...                  1   4722365022   \n",
       "324886 -0.861629 -0.507538  ...                  1   1122466035   \n",
       "324887 -0.861629 -0.507538  ...                  1  24347000000   \n",
       "\n",
       "        log_shares_out  eps_surp_pct_final  div_amount  \\\n",
       "0            20.538016                0.00         0.0   \n",
       "1            21.203306                2.04         0.0   \n",
       "2            19.958210                0.00         0.0   \n",
       "3            20.825254                0.00         0.0   \n",
       "4            21.639557                0.00         0.0   \n",
       "...                ...                 ...         ...   \n",
       "324883       20.495916                0.00         0.0   \n",
       "324884       21.213859                0.00         0.0   \n",
       "324885       22.275576                0.00         0.0   \n",
       "324886       20.838794                0.00         0.0   \n",
       "324887       23.915674                0.00         0.0   \n",
       "\n",
       "        log_shares_out_iqr_outlier  eps_estimate_rz_8  eps_actual   lret_1m  \\\n",
       "0                                0                0.0        0.00 -0.000199   \n",
       "1                                0                0.0        0.62  0.000314   \n",
       "2                                0                0.0        0.00 -0.000334   \n",
       "3                                0                0.0        0.00  0.000698   \n",
       "4                                0                0.0        0.00 -0.000769   \n",
       "...                            ...                ...         ...       ...   \n",
       "324883                           0                0.0        0.00 -0.000461   \n",
       "324884                           0                0.0        0.00 -0.000698   \n",
       "324885                           0                0.0        0.00  0.000161   \n",
       "324886                           0                0.0        0.00  0.000270   \n",
       "324887                           0                0.0        0.00  0.000348   \n",
       "\n",
       "        y_target  \n",
       "0      -0.000100  \n",
       "1      -0.000126  \n",
       "2       0.000721  \n",
       "3      -0.000698  \n",
       "4      -0.000907  \n",
       "...          ...  \n",
       "324883 -0.000132  \n",
       "324884 -0.001316  \n",
       "324885  0.000375  \n",
       "324886 -0.000135  \n",
       "324887 -0.000025  \n",
       "\n",
       "[324888 rows x 191 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== 1) Data Preparation & Target Construction =====\n",
    "# Make a working copy of the original dataset\n",
    "df = df_ori.copy()\n",
    "\n",
    "# Sort by symbol and datetime\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values(by=['symbol', 'datetime'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# --- Compute 1-minute log return for each symbol ---\n",
    "# lret_1m = log(price_t / price_{t-1})\n",
    "df['lret_1m'] = df.groupby('symbol')['close'].transform(lambda s: np.log(s).diff())\n",
    "\n",
    "# --- Define prediction target: next-period log return ---\n",
    "# y_target = log-return shifted by one step ahead (t+1)\n",
    "df['y_target'] = df.groupby('symbol')['lret_1m'].shift(-1)\n",
    "\n",
    "\n",
    "# --- Remove outliers and missing values ---\n",
    "initial_rows = len(df)\n",
    "# Filter extreme targets |y| > 0.2 (to avoid abnormal spikes)\n",
    "df = df[(df['y_target'].abs() <= 0.2)]\n",
    "# Drop NaN values in target or feature columns\n",
    "df = df.dropna(subset=['y_target', 'lret_1m'])\n",
    "\n",
    "print(f\"(Dropped {initial_rows - len(df)} unusal/NaN y value)\")\n",
    "\n",
    "# --- Re-sort by datetime and symbol for time-series consistency ---\n",
    "df = df.sort_values(by=['datetime', 'symbol'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3dd2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X original shape: (324888, 184)\n"
     ]
    }
   ],
   "source": [
    "# ===== 2) Feature Cleaning & Target / Feature Split =====\n",
    "# --- Separate target variable ---\n",
    "y = df['y_target']\n",
    "\n",
    "# --- Drop columns not used for modeling ---\n",
    "X = df.drop(columns=[\n",
    "    'y_target', 'lret_1m', 'datetime', 'symbol',  \n",
    "    'year', 'month', 'day', 'minute', 'minute_of_day'\n",
    "], errors='ignore')\n",
    "\n",
    "print(f\"X original shape: {X.shape}\")\n",
    "\n",
    "# --- Identify problematic columns based on summary statistics ---\n",
    "desc = X.describe(percentiles=[0.99]).T\n",
    "bad_cols = []\n",
    "\n",
    "# std > 1000 → extreme scale / noise\n",
    "bad_cols += desc.index[desc['std'] > 1e3].tolist()\n",
    "# 99th percentile > 1000 → heavy-tailed distribution\n",
    "bad_cols += desc.index[desc['99%'].abs() > 1e3].tolist()\n",
    "# max > 1,000,000 → extreme outliers\n",
    "bad_cols += desc.index[desc['max'].abs() > 1e6].tolist()\n",
    "# std == 0 → constant / non-informative columns\n",
    "bad_cols += desc.index[desc['std'] == 0].tolist()\n",
    "\n",
    "bad_cols_set = sorted(set(bad_cols))\n",
    "#print(f\"--- Dropping {len(bad_cols_set)} bad cols ---\")\n",
    "#for col in bad_cols_set:\n",
    "#   print(f\"  - {col}\")\n",
    "\n",
    "# --- Drop the identified bad columns ---\n",
    "X_cleaned = X.drop(columns=bad_cols_set)\n",
    "\n",
    "#print(f\"\\n--- X cleaned ---\")\n",
    "#print(f\"X (cleaned) shape: {X_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e723c91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Splitting done ---\n",
      " Validation set (X_val, y_val) shape: (216592, 147), (216592,)\n",
      " Testing set (X_test, y_test) shape: (108296, 147), (108296,)\n"
     ]
    }
   ],
   "source": [
    "# ===== 3) Time Series Split (first 2/3 for training + last 1/3 for testing) =====\n",
    "split_ratio = 1.0 / 1.5 \n",
    "split_index = int(len(X_cleaned) * split_ratio)\n",
    "\n",
    "X_val = X_cleaned.iloc[:split_index]\n",
    "y_val = y.iloc[:split_index]\n",
    "\n",
    "X_test = X_cleaned.iloc[split_index:]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "print(f\"--- Data Splitting done ---\")\n",
    "print(f\" Validation set (X_val, y_val) shape: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\" Testing set (X_test, y_test) shape: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "05aa4dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Time Range ===\n",
      "Training period: 2024-04-30 12:51:00 → 2025-04-29 15:59:00\n",
      "Testing period: 2025-04-29 15:59:00 → 2025-10-28 15:58:00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Time Range ===\")\n",
    "print(f\"Training period: {df.loc[X_train.index, 'datetime'].min()} → {df.loc[X_train.index, 'datetime'].max()}\")\n",
    "print(f\"Testing period: {df.loc[X_test.index, 'datetime'].min()} → {df.loc[X_test.index, 'datetime'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb6126",
   "metadata": {},
   "source": [
    "# LightGBM (5-Feature Variant) — Rolling Backtest\n",
    "\n",
    "### Purpose\n",
    "This notebook documents an **ablation experiment** using only **five manually selected features** to test whether a smaller and more interpretable model could reduce overfitting and improve stability compared with the full Lasso-selected feature set.\n",
    "\n",
    "### Setup\n",
    "- **Model:** LightGBM regressor (same parameters as the main run)\n",
    "- **Window:** Rolling training with a 30-minute lookback  \n",
    "- **Data:** Same test period and symbols as the main Lasso-feature model  \n",
    "- **Feature set:** Fixed to 5 preselected predictors  \n",
    "- **Tuning:** No hyperparameter optimization (reusing best parameters from the main notebook)\n",
    "\n",
    "### Results Summary\n",
    "| Metric | Full Lasso Feature Model | 5-Feature Model | Comparison |\n",
    "|:--|:--|:--|:--|\n",
    "| Overall IC (sample-wise) | **0.0037** | **-0.0106** | ↓ weaker directional consistency |\n",
    "| Sharpe (sign, minute-level) | **-0.0088** | **-0.0040** | ↔ both near zero |\n",
    "| Sharpe (long-only, minute-level) | **0.0207** | **0.0172** | ↓ slightly lower |\n",
    "| Interpretation | Richer feature space → higher predictive power | Simpler but less expressive |  |\n",
    "\n",
    "### Interpretation\n",
    "While the 5-feature version simplifies the model and improves runtime efficiency, it fails to capture the same predictive strength as the full Lasso feature model.  \n",
    "LightGBM benefits from a broader set of weak but complementary predictors; trimming the features overly restricts model flexibility and signal diversity.\n",
    "\n",
    "### Decision\n",
    "The **5-feature LightGBM model is not selected** for the final portfolio evaluation.  \n",
    "It is retained here for **transparency, reproducibility, and future audits** as part of the ablation study.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ab8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prepared X_val_lasso / y_val. Index type: RangeIndex\n"
     ]
    }
   ],
   "source": [
    "# ========= Inputs you already have =========\n",
    "# X_lasso: DataFrame (your 75-feature subset)\n",
    "# y      : Series (aligned with X_lasso by index)\n",
    "\n",
    "# [OPTIONAL] If you have a datetime Series aligned with X_lasso (same index), set it here:\n",
    "datetime_series = None\n",
    "# e.g. if you still have a df with datetime: datetime_series = df.loc[X_lasso.index, \"datetime\"]\n",
    "\n",
    "# ========= Build canonical base (no other df needed) =========\n",
    "base = X_lasso.copy()\n",
    "base_ycol = \"y_target\"\n",
    "base[base_ycol] = y.values  # align by position (indices already aligned)\n",
    "\n",
    "# attach datetime if provided\n",
    "if datetime_series is not None:\n",
    "    base[\"datetime\"] = pd.to_datetime(datetime_series)\n",
    "    base = base.set_index(\"datetime\").sort_index()\n",
    "\n",
    "# ========= Split (2/3 Train+Val, 1/3 Test) =========\n",
    "split_ratio = 1.0 / 1.5\n",
    "split_index = int(len(base) * split_ratio)\n",
    "\n",
    "X_val_lasso  = base.drop(columns=[base_ycol]).iloc[:split_index].copy()\n",
    "y_val        = base[base_ycol].iloc[:split_index].copy()\n",
    "X_test_lasso = base.drop(columns=[base_ycol]).iloc[split_index:].copy()\n",
    "y_test       = base[base_ycol].iloc[split_index:].copy()\n",
    "\n",
    "print(\"✅ Prepared X_val_lasso / y_val. Index type:\", type(X_val_lasso.index).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe9c14a",
   "metadata": {},
   "source": [
    "1. IC Stability Test (Rolling Spearman ρ Stability Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0e821981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: feature_ic_stability.csv\n"
     ]
    }
   ],
   "source": [
    "# ========= IC stability =========\n",
    "def ic_stability_monthly(X: pd.DataFrame, y: pd.Series, min_samples=200) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for feat in X.columns:\n",
    "        vals = []\n",
    "        for _, Xm in X.groupby(pd.Grouper(freq=\"M\")):\n",
    "            if len(Xm) < min_samples: \n",
    "                continue\n",
    "            ym = y.loc[Xm.index]\n",
    "            v  = pd.concat([Xm[feat], ym], axis=1).dropna()\n",
    "            if len(v) < min_samples or v.iloc[:,0].nunique() < 2: \n",
    "                continue\n",
    "            vals.append(spearmanr(v.iloc[:,0].values, v.iloc[:,1].values, nan_policy=\"omit\")[0])\n",
    "        if len(vals)==0:\n",
    "            rows.append({\"feature\": feat, \"n_windows\": 0, \"mean_ic\": np.nan, \"std_ic\": np.nan, \"sign_consistency\": np.nan})\n",
    "        else:\n",
    "            arr = np.array(vals, float)\n",
    "            rows.append({\n",
    "                \"feature\": feat,\n",
    "                \"n_windows\": len(arr),\n",
    "                \"mean_ic\": float(np.nanmean(arr)),\n",
    "                \"std_ic\": float(np.nanstd(arr)),\n",
    "                \"sign_consistency\": float(np.mean(arr > 0)),\n",
    "            })\n",
    "    return pd.DataFrame(rows).sort_values(\"mean_ic\", ascending=False)\n",
    "\n",
    "def ic_stability_chunks(X: pd.DataFrame, y: pd.Series, chunk_size=5000) -> pd.DataFrame:\n",
    "    rows, n = [], len(X)\n",
    "    cuts = list(range(0, n, chunk_size)) + [n]\n",
    "    for feat in X.columns:\n",
    "        vals = []\n",
    "        for i in range(len(cuts)-1):\n",
    "            s, e = cuts[i], cuts[i+1]\n",
    "            Xm = X.iloc[s:e, [X.columns.get_loc(feat)]]\n",
    "            ym = y.iloc[s:e]\n",
    "            v  = pd.concat([Xm.iloc[:,0], ym], axis=1).dropna()\n",
    "            if len(v) < max(200, chunk_size//2) or v.iloc[:,0].nunique() < 2:\n",
    "                continue\n",
    "            vals.append(spearmanr(v.iloc[:,0].values, v.iloc[:,1].values, nan_policy=\"omit\")[0])\n",
    "        if len(vals)==0:\n",
    "            rows.append({\"feature\": feat, \"n_windows\": 0, \"mean_ic\": np.nan, \"std_ic\": np.nan, \"sign_consistency\": np.nan})\n",
    "        else:\n",
    "            arr = np.array(vals, float)\n",
    "            rows.append({\n",
    "                \"feature\": feat,\n",
    "                \"n_windows\": len(arr),\n",
    "                \"mean_ic\": float(np.nanmean(arr)),\n",
    "                \"std_ic\": float(np.nanstd(arr)),\n",
    "                \"sign_consistency\": float(np.mean(arr > 0)),\n",
    "            })\n",
    "    return pd.DataFrame(rows).sort_values(\"mean_ic\", ascending=False)\n",
    "\n",
    "if isinstance(X_val_lasso.index, pd.DatetimeIndex):\n",
    "    ic_df = ic_stability_monthly(X_val_lasso, y_val)\n",
    "else:\n",
    "    ic_df = ic_stability_chunks(X_val_lasso, y_val, chunk_size=5000)\n",
    "\n",
    "ic_df.to_csv(\"feature_ic_stability.csv\", index=False)\n",
    "print(\"✅ Saved: feature_ic_stability.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49790b57",
   "metadata": {},
   "source": [
    "2. Feature Importance (by Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "77405218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: feature_gain_importance.csv\n",
      "✅ Saved: feature_ic_gain_merged.csv\n",
      "⚠️ No features pass stability thresholds. Falling back to top-K by gain only.\n",
      "✅ Saved: selected_features_ic_gain.csv (n=30)\n"
     ]
    }
   ],
   "source": [
    "# ========= Gain importance (train once on X_val_lasso, y_val) =========\n",
    "BEST_PARAMS_LGBM = {\n",
    "    \"num_leaves\": 63, \"min_child_samples\": 50, \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9, \"reg_lambda\": 1.0, \"reg_alpha\": 0.0\n",
    "}\n",
    "\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_imp = pd.DataFrame(imp.fit_transform(X_val_lasso), index=X_val_lasso.index, columns=X_val_lasso.columns)\n",
    "\n",
    "lgbm = LGBMRegressor(\n",
    "    objective=\"regression\", n_estimators=1000, learning_rate=0.05,\n",
    "    random_state=42, n_jobs=-1, verbosity=-1, force_row_wise=True,\n",
    "    **BEST_PARAMS_LGBM\n",
    ").fit(X_imp, y_val)\n",
    "\n",
    "gain = lgbm.booster_.feature_importance(importance_type=\"gain\")\n",
    "gain_df = pd.DataFrame({\"feature\": X_val_lasso.columns, \"gain\": gain}).sort_values(\"gain\", ascending=False)\n",
    "gain_df.to_csv(\"feature_gain_importance.csv\", index=False)\n",
    "print(\"✅ Saved: feature_gain_importance.csv\")\n",
    "\n",
    "# ========= Merge & select =========\n",
    "THRESH_MEAN_IC = 0.02\n",
    "THRESH_SIGN    = 0.60\n",
    "TOP_K_BY_GAIN  = 30\n",
    "\n",
    "merged = gain_df.merge(ic_df, on=\"feature\", how=\"left\")\n",
    "merged.to_csv(\"feature_ic_gain_merged.csv\", index=False)\n",
    "print(\"✅ Saved: feature_ic_gain_merged.csv\")\n",
    "\n",
    "stable = merged[\n",
    "    (merged[\"mean_ic\"].fillna(-1) > THRESH_MEAN_IC) &\n",
    "    (merged[\"sign_consistency\"].fillna(0) > THRESH_SIGN)\n",
    "].sort_values(\"gain\", ascending=False)\n",
    "\n",
    "if stable.empty:\n",
    "    print(\"⚠️ No features pass stability thresholds. Falling back to top-K by gain only.\")\n",
    "    selected = gain_df.head(TOP_K_BY_GAIN)[\"feature\"].tolist()\n",
    "else:\n",
    "    selected = stable.head(TOP_K_BY_GAIN)[\"feature\"].tolist()\n",
    "\n",
    "pd.DataFrame({\"feature\": selected}).to_csv(\"selected_features_ic_gain.csv\", index=False)\n",
    "print(f\"✅ Saved: selected_features_ic_gain.csv (n={len(selected)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aac068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: feature_count_selection_cv_results.csv\n",
      "    K  Sharpe_step_concat   IC_mean    IC_std  MSE_mean   R2_mean  pnl_len\n",
      "7  75            0.005904  0.009297  0.011091  0.000008 -0.145659   196900\n",
      "6  60            0.005056  0.007018  0.009052  0.000008 -0.136600   196900\n",
      "3  30            0.003016  0.007525  0.008921  0.000008 -0.153262   196900\n",
      "5  50            0.002524  0.004087  0.010204  0.000008 -0.140766   196900\n",
      "0  10            0.002444 -0.001622  0.010282  0.000008 -0.168871   196900\n",
      "4  40            0.001823  0.006682  0.011544  0.000008 -0.142505   196900\n",
      "2  20            0.000886 -0.001647  0.008310  0.000008 -0.155124   196900\n",
      "1  15            0.000271  0.000648  0.009061  0.000008 -0.161526   196900\n",
      "\n",
      "✅ Best K by CV per-step Sharpe: K=75 (Sharpe=0.005904, IC_mean=0.009297)\n",
      "\n",
      "=== [OOS • Test Split with Best-K] ===\n",
      "K = 75\n",
      "Sharpe (per-step, sign): 0.005611\n",
      "IC (Spearman)         : 0.017103\n",
      "MSE                    : 0.00000587\n",
      "R²                     : -0.382233\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# ==============================\n",
    "# 0) Inputs & Defaults\n",
    "# ==============================\n",
    "# Required:  X_val_lasso, y_val  must already exist (same as before)\n",
    "# Optional:  X_test_lasso, y_test  if available, a final test evaluation will be run using the best K\n",
    "# Required:  feature_gain_importance.csv  (generated from the previous step)\n",
    "gain_df = pd.read_csv(\"feature_gain_importance.csv\")\n",
    "feature_order = gain_df.sort_values(\"gain\", ascending=False)[\"feature\"].tolist()\n",
    "\n",
    "K_list = [10, 15, 20, 30, 40, 50, 60, 75]  \n",
    "K_list = [k for k in K_list if k <= len(feature_order)]\n",
    "\n",
    "def sharpe_from_pnl(pnl: pd.Series) -> float:\n",
    "    std = pnl.std(ddof=1)\n",
    "    return 0.0 if (std == 0 or np.isnan(std)) else pnl.mean() / std\n",
    "\n",
    "def fold_pnl_sign_no_agg(idx, y_true, y_pred):\n",
    "    signals = np.sign(y_pred)\n",
    "    return pd.Series(signals * y_true, index=idx)\n",
    "\n",
    "BEST_PARAMS_LGBM = {\n",
    "    \"num_leaves\": 63, \"min_child_samples\": 50,\n",
    "    \"subsample\": 0.9, \"colsample_bytree\": 0.9,\n",
    "    \"reg_lambda\": 1.0, \"reg_alpha\": 0.0\n",
    "}\n",
    "\n",
    "def build_lgbm_pipe(params: dict):\n",
    "    return Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"reg\", LGBMRegressor(\n",
    "            objective=\"regression\",\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1,\n",
    "            force_row_wise=True,\n",
    "            **params\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# ==============================\n",
    "# 1) CV over K (TimeSeriesSplit=10)\n",
    "# ==============================\n",
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "rows = []\n",
    "\n",
    "for K in K_list:\n",
    "    feats = feature_order[:K]\n",
    "    Xk = X_val_lasso[feats]\n",
    "\n",
    "    all_fold_pnl = []\n",
    "    ic_list, mse_list, r2_list = [], [], []\n",
    "\n",
    "    for tr_idx, va_idx in tscv.split(Xk):\n",
    "        X_tr, y_tr = Xk.iloc[tr_idx], y_val.iloc[tr_idx]\n",
    "        X_va, y_va = Xk.iloc[va_idx], y_val.iloc[va_idx]\n",
    "\n",
    "        pipe = build_lgbm_pipe(BEST_PARAMS_LGBM)\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        y_hat = pipe.predict(X_va)\n",
    "\n",
    "        # per-step sign Sharpe\n",
    "        fold_pnl = fold_pnl_sign_no_agg(Xk.index[va_idx], y_va.values, y_hat)\n",
    "        all_fold_pnl.append(fold_pnl)\n",
    "\n",
    "        try:\n",
    "            ic = spearmanr(y_va.values, y_hat, nan_policy=\"omit\")[0]\n",
    "        except Exception:\n",
    "            ic = np.nan\n",
    "        ic_list.append(ic)\n",
    "        mse_list.append(mean_squared_error(y_va.values, y_hat))\n",
    "        r2_list.append(r2_score(y_va.values, y_hat))\n",
    "\n",
    "    pnl_concat = pd.concat(all_fold_pnl).sort_index()\n",
    "    sr_step = sharpe_from_pnl(pnl_concat)\n",
    "\n",
    "    rows.append({\n",
    "        \"K\": K,\n",
    "        \"Sharpe_step_concat\": float(sr_step),\n",
    "        \"IC_mean\": float(np.nanmean(ic_list)),\n",
    "        \"IC_std\": float(np.nanstd(ic_list)),\n",
    "        \"MSE_mean\": float(np.mean(mse_list)),\n",
    "        \"R2_mean\": float(np.mean(r2_list)),\n",
    "        \"pnl_len\": int(pnl_concat.size)\n",
    "    })\n",
    "\n",
    "cv_table = pd.DataFrame(rows).sort_values(\"Sharpe_step_concat\", ascending=False)\n",
    "cv_table.to_csv(\"feature_count_selection_cv_results.csv\", index=False)\n",
    "print(\"✅ Saved: feature_count_selection_cv_results.csv\")\n",
    "print(cv_table)\n",
    "\n",
    "best_row = cv_table.iloc[0]\n",
    "best_K = int(best_row[\"K\"])\n",
    "print(f\"\\n✅ Best K by CV per-step Sharpe: K={best_K} (Sharpe={best_row['Sharpe_step_concat']:.6f}, \"\n",
    "      f\"IC_mean={best_row['IC_mean']:.6f})\")\n",
    "\n",
    "# ==============================\n",
    "# 2) (Optional) Final test on X_test with best K\n",
    "# ==============================\n",
    "if 'X_test_lasso' in globals() and 'y_test' in globals():\n",
    "    feats = feature_order[:best_K]\n",
    "    Xtr, ytr = X_val_lasso[feats], y_val\n",
    "    Xte, yte = X_test_lasso[feats], y_test\n",
    "\n",
    "    pipe = build_lgbm_pipe(BEST_PARAMS_LGBM)\n",
    "    pipe.fit(Xtr, ytr)\n",
    "    yhat_te = pipe.predict(Xte)\n",
    "\n",
    "    # sample-level Sharpe\n",
    "    pnl_te = fold_pnl_sign_no_agg(Xte.index, yte.values, yhat_te)\n",
    "    sharpe_te = sharpe_from_pnl(pnl_te)\n",
    "\n",
    "    try:\n",
    "        ic_te = spearmanr(yte.values, yhat_te, nan_policy=\"omit\")[0]\n",
    "    except Exception:\n",
    "        ic_te = np.nan\n",
    "    mse_te = mean_squared_error(yte.values, yhat_te)\n",
    "    r2_te  = r2_score(yte.values, yhat_te)\n",
    "\n",
    "    print(\"\\n=== [OOS • Test Split with Best-K] ===\")\n",
    "    print(f\"K = {best_K}\")\n",
    "    print(f\"Sharpe (per-step, sign): {sharpe_te:.6f}\")\n",
    "    print(f\"IC (Spearman)         : {ic_te:.6f}\")\n",
    "    print(f\"MSE                    : {mse_te:.8f}\")\n",
    "    print(f\"R²                     : {r2_te:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d208bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Strategy used: A: adaptive_thresholds\n",
      "✅ Saved: selected_features_adaptive.csv (n=5)\n",
      "                    feature   mean_ic    std_ic  sign_consistency      gain  \\\n",
      "0                    vol_60  0.008047  0.022259          0.604651  0.547870   \n",
      "8   fz_spill_AMAT_lret_lag3  0.006110  0.022549          0.627907  0.363969   \n",
      "15            f_volume_norm  0.007787  0.019776          0.604651  0.295219   \n",
      "43                fz_vol_60  0.005210  0.021358          0.651163  0.126349   \n",
      "44          fz_vol_ratio_15  0.002069  0.018376          0.627907  0.114648   \n",
      "\n",
      "    gain_norm  stability_score  \n",
      "0    1.000000         0.797874  \n",
      "8    0.664334         0.708744  \n",
      "15   0.538848         0.660025  \n",
      "43   0.230619         0.590496  \n",
      "44   0.209261         0.573057  \n"
     ]
    }
   ],
   "source": [
    "# --- Feature Filtering ---\n",
    "\n",
    "df = pd.read_csv(\"feature_ic_gain_merged.csv\").copy()\n",
    "\n",
    "# --- Cleaning & Derivation ---\n",
    "for col in [\"mean_ic\",\"std_ic\",\"sign_consistency\",\"gain\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Normalize feature gain\n",
    "max_gain = df[\"gain\"].max() if np.isfinite(df[\"gain\"]).any() else 1.0\n",
    "df[\"gain_norm\"] = df[\"gain\"].fillna(0.0) / (max_gain if max_gain != 0 else 1.0)\n",
    "\n",
    "# Handle missing values (avoid all-NaN crashes)\n",
    "df[\"mean_ic\"] = df[\"mean_ic\"].fillna(df[\"mean_ic\"].median() if np.isfinite(df[\"mean_ic\"]).any() else 0.0)\n",
    "df[\"std_ic\"]  = df[\"std_ic\"].fillna(df[\"std_ic\"].median()  if np.isfinite(df[\"std_ic\"]).any()  else 0.5)\n",
    "df[\"sign_consistency\"] = df[\"sign_consistency\"].fillna(0.5)\n",
    "\n",
    "# Compute overall stability score (with adjustable weights)\n",
    "w_sign, w_gain, w_icstd = 0.5, 0.3, 0.2\n",
    "df[\"stability_score\"] = (\n",
    "    w_sign * df[\"sign_consistency\"] +\n",
    "    w_gain * df[\"gain_norm\"] +\n",
    "    w_icstd * (1 - df[\"std_ic\"])\n",
    ")\n",
    "\n",
    "# ===== Strategy A: Adaptive threshold (quantile-based) =====\n",
    "# Goal: consistent direction, meaningful contribution, and acceptable volatility\n",
    "q_sign = df[\"sign_consistency\"].quantile(0.60)\n",
    "q_gain = df[\"gain_norm\"].quantile(0.40)\n",
    "q_icstd= df[\"std_ic\"].quantile(0.80)\n",
    "\n",
    "SIGN_THRESHOLD = max(0.60, q_sign)        # - At least 60% directional consistency or above 60th percentile of all samples\n",
    "GAIN_MIN      = max(0.02, q_gain)         # - At least top 60% in normalized gain (gain_norm)\n",
    "IC_MEAN_MIN   = max(-0.002, df[\"mean_ic\"].quantile(0.40))  # - Allow slightly near-zero mean IC\n",
    "IC_STD_MAX    = min(0.50, q_icstd)        # - Relaxed volatility constraint (upper bound)\n",
    "\n",
    "selA = df.query(\n",
    "    \"sign_consistency >= @SIGN_THRESHOLD and \"\n",
    "    \"gain_norm >= @GAIN_MIN and \"\n",
    "    \"mean_ic >= @IC_MEAN_MIN and \"\n",
    "    \"std_ic <= @IC_STD_MAX\"\n",
    ").sort_values([\"stability_score\",\"gain_norm\"], ascending=False)\n",
    "\n",
    "# ===== Strategy B: If no features pass → use Top-K by stability score =====\n",
    "TOP_K = 30\n",
    "if selA.empty:\n",
    "    selB = df.sort_values([\"stability_score\",\"gain_norm\"], ascending=False).head(TOP_K)\n",
    "else:\n",
    "    selB = selA\n",
    "\n",
    "# ===== Strategy C: If still empty (extreme case) → fallback to Top-K by gain =====\n",
    "if selB.empty:\n",
    "    selC = df.sort_values(\"gain\", ascending=False).head(TOP_K)\n",
    "    used_strategy = \"C: gain_topK\"\n",
    "    final = selC\n",
    "elif len(selB) > TOP_K:\n",
    "    used_strategy = \"B: stability_score_topK\"\n",
    "    final = selB.head(TOP_K)\n",
    "else:\n",
    "    used_strategy = \"A: adaptive_thresholds\"\n",
    "    final = selB\n",
    "\n",
    "final = final[[\"feature\",\"mean_ic\",\"std_ic\",\"sign_consistency\",\"gain\",\"gain_norm\",\"stability_score\"]]\n",
    "final.to_csv(\"selected_features_adaptive.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Strategy used: {used_strategy}\")\n",
    "print(f\"✅ Saved: selected_features_adaptive.csv (n={len(final)})\")\n",
    "print(final.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1eaf16ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_core successfully generated: shape = (324888, 5) (containing 5 features).\n"
     ]
    }
   ],
   "source": [
    "# ===== Stable Core 5 Features (from adaptive selection) =====\n",
    "LGBM_CORE_FEATURES = [\n",
    "    \"vol_60\",\n",
    "    \"fz_spill_AMAT_lret_lag3\",\n",
    "    \"f_volume_norm\",\n",
    "    \"fz_vol_60\",\n",
    "    \"fz_vol_ratio_15\"\n",
    "]\n",
    "\n",
    "# 2) Construct X_core based on X_cleaned\n",
    "present = [c for c in LGBM_CORE_FEATURES if c in X_cleaned.columns]\n",
    "missing = [c for c in LGBM_CORE_FEATURES if c not in X_cleaned.columns]\n",
    "\n",
    "if missing:\n",
    "    print(f\"{len(missing)} Core features are missing from X_cleaned (not included in X_core).\")\n",
    "    for c in missing:\n",
    "        print(\"   -\", c)\n",
    "\n",
    "X_core = X_cleaned.loc[:, present].copy()\n",
    "\n",
    "# X_lasso = X_lasso.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "print(f\"X_core successfully generated: shape = {X_core.shape} (containing {len(present)} features).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Splitting done ---\n",
      " Validation set (X_val, y_val): (216592, 5), (216592,)\n",
      " Testing set (X_test, y_test):   (108296, 5), (108296,)\n"
     ]
    }
   ],
   "source": [
    "split_ratio = 1.0 / 1.5 \n",
    "split_index = int(len(X_core) * split_ratio)\n",
    "\n",
    "X_val_core = X_core.iloc[:split_index]\n",
    "y_val = y.iloc[:split_index]\n",
    "\n",
    "X_test_core = X_core.iloc[split_index:]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "print(f\"--- Data Splitting done ---\")\n",
    "print(f\" Validation set (X_val, y_val): {X_val_core.shape}, {y_val.shape}\")\n",
    "print(f\" Testing set (X_test, y_test):   {X_test_core.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a4b2a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Configuration =====\n",
    "ROLLING_WINDOW_MINUTES = 30\n",
    "\n",
    "BEST_PARAMS_LGBM = {  # Best parameters from CV\n",
    "    \"num_leaves\": 63,\n",
    "    \"min_child_samples\": 50,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"reg_alpha\": 0.0,\n",
    "}\n",
    "\n",
    "def build_lgbm_pipe(best_params=BEST_PARAMS_LGBM):\n",
    "    return Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"reg\", LGBMRegressor(\n",
    "            objective=\"regression\",\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1,\n",
    "            force_row_wise=True,\n",
    "            **best_params\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def sharpe_per_step(series: pd.Series) -> float:\n",
    "    std = series.std(ddof=1)\n",
    "    return 0.0 if (std == 0 or np.isnan(std)) else series.mean() / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "eb39aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting minute-level rolling backtest (LGBM + Lasso) ---\n",
      "Testing period: 2025-04-29T15:59:00.000000000 → 2025-10-28T15:58:00.000000000\n",
      "Rolling window (training): 30 minutes\n",
      "LightGBM best parameters: {'num_leaves': 63, 'min_child_samples': 50, 'subsample': 0.9, 'colsample_bytree': 0.9, 'reg_lambda': 1.0, 'reg_alpha': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# ===== 1) Prepare backtest data =====\n",
    "df_backtest = df.set_index('datetime')\n",
    "\n",
    "test_original_indices = X_test_core.index\n",
    "test_datetimes = np.sort(df.loc[test_original_indices, 'datetime'].unique())\n",
    "\n",
    "feature_cols = list(X_val_core.columns)\n",
    "available_cols = [c for c in feature_cols if c in df_backtest.columns]\n",
    "if len(available_cols) < len(feature_cols):\n",
    "    miss = sorted(set(feature_cols) - set(available_cols))\n",
    "    print(f\"⚠️ Missing {len(miss)} Lasso features: {miss[:10]}{' ...' if len(miss)>10 else ''}\")\n",
    "\n",
    "X_core_full_history = df_backtest[available_cols]\n",
    "y_full_history = df_backtest['y_target']\n",
    "meta_full_history = df_backtest[['symbol']]\n",
    "\n",
    "print(f\"--- Starting minute-level rolling backtest (LGBM + Lasso) ---\")\n",
    "print(f\"Testing period: {test_datetimes.min()} → {test_datetimes.max()}\")\n",
    "print(f\"Rolling window (training): {ROLLING_WINDOW_MINUTES} minutes\")\n",
    "print(f\"LightGBM best parameters: {BEST_PARAMS_LGBM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e2ce0a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21660/21660 [1:41:59<00:00,  3.54it/s]   \n"
     ]
    }
   ],
   "source": [
    "# ===== 2) Rolling window: [t-30m, t-1m] → predict t =====\n",
    "results = []\n",
    "\n",
    "for current_dt in tqdm(test_datetimes):\n",
    "    train_start_dt = current_dt - pd.Timedelta(minutes=ROLLING_WINDOW_MINUTES)\n",
    "    train_end_dt   = current_dt - pd.Timedelta(minutes=1)\n",
    "\n",
    "    # Training window\n",
    "    X_tr = X_core_full_history.loc[train_start_dt:train_end_dt]\n",
    "    y_tr = y_full_history.loc[train_start_dt:train_end_dt]\n",
    "\n",
    "    if len(X_tr) < 15 * 5:\n",
    "        continue\n",
    "\n",
    "    # Test data at current minute\n",
    "    X_te = X_core_full_history.loc[current_dt]\n",
    "    if isinstance(X_te, pd.Series):\n",
    "        X_te = X_te.to_frame().T\n",
    "\n",
    "    pipe = build_lgbm_pipe()\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    y_hat = pipe.predict(X_te)\n",
    "\n",
    "    meta_rows = meta_full_history.loc[current_dt]\n",
    "    y_true = y_full_history.loc[current_dt]\n",
    "\n",
    "    if isinstance(meta_rows, pd.Series):\n",
    "        results.append({\n",
    "            \"datetime\": current_dt,\n",
    "            \"symbol\": meta_rows['symbol'],\n",
    "            \"predicted_log_return\": float(y_hat[0]),\n",
    "            \"actual_log_return\": float(y_true),\n",
    "        })\n",
    "    else:\n",
    "        for i in range(len(meta_rows)):\n",
    "            results.append({\n",
    "                \"datetime\": current_dt,\n",
    "                \"symbol\": meta_rows.iloc[i]['symbol'],\n",
    "                \"predicted_log_return\": float(y_hat[i]),\n",
    "                \"actual_log_return\": float(y_true.iloc[i]),\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "297413ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Backtesting completed ---\n",
      "Total backtest rows: 99749\n",
      "             datetime symbol  predicted_log_return  actual_log_return  \\\n",
      "0 2025-04-29 15:59:00   AMAT             -0.000565          -0.014007   \n",
      "1 2025-04-29 15:59:00    AMD             -0.000243          -0.006581   \n",
      "2 2025-04-29 15:59:00   AVGO             -0.000243          -0.008008   \n",
      "3 2025-04-29 15:59:00     MU             -0.000243          -0.019444   \n",
      "4 2025-04-29 15:59:00   NVDA             -0.000243          -0.025525   \n",
      "\n",
      "   weight_relative  weight_sign  pnl_sample_sign  pnl_sample_relative  \n",
      "0              0.0         -1.0         0.014007                 -0.0  \n",
      "1              0.0         -1.0         0.006581                 -0.0  \n",
      "2              0.0         -1.0         0.008008                 -0.0  \n",
      "3              0.0         -1.0         0.019444                 -0.0  \n",
      "4              0.0         -1.0         0.025525                 -0.0  \n"
     ]
    }
   ],
   "source": [
    "# ===== 3) Post-processing: derived columns =====\n",
    "results_df_core = pd.DataFrame(results).sort_values([\"datetime\",\"symbol\"]).reset_index(drop=True)\n",
    "\n",
    "# (A) Long-only relative weights (sum=1 per minute)\n",
    "results_df_core[\"positive_prediction\"] = results_df_core[\"predicted_log_return\"].clip(lower=0)\n",
    "minute_sum = results_df_core.groupby(\"datetime\")[\"positive_prediction\"].transform(\"sum\")\n",
    "results_df_core[\"weight_relative\"] = np.where(minute_sum == 0, 0.0,\n",
    "                                         results_df_core[\"positive_prediction\"] / minute_sum)\n",
    "results_df_core.drop(columns=[\"positive_prediction\"], inplace=True)\n",
    "\n",
    "# (B) Sign weights (-1/0/+1)\n",
    "results_df_core[\"weight_sign\"] = np.sign(results_df_core[\"predicted_log_return\"])\n",
    "\n",
    "# (C) Per-sample PnL (sign-based, same as CV)\n",
    "results_df_core[\"pnl_sample_sign\"] = results_df_core[\"weight_sign\"] * results_df_core[\"actual_log_return\"]\n",
    "\n",
    "# (D) Long-only PnL (relative weights)\n",
    "results_df_core[\"pnl_sample_relative\"] = results_df_core[\"weight_relative\"] * results_df_core[\"actual_log_return\"]\n",
    "\n",
    "# (E) Aggregate by minute (optional)\n",
    "pnl_minute_sign = results_df_core.groupby(\"datetime\")[\"pnl_sample_sign\"].mean()\n",
    "pnl_minute_relative = results_df_core.groupby(\"datetime\")[\"pnl_sample_relative\"].sum()\n",
    "\n",
    "print(\"\\n--- Backtesting completed ---\")\n",
    "print(f\"Total backtest rows: {len(results_df_core)}\")\n",
    "print(results_df_core.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8ee42a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [OOS • Test Split] Performance ===\n",
      "MSE                         : 0.00000463\n",
      "R²                          : -0.012640\n",
      "IC (Spearman ρ)             : -0.010608\n",
      "Sharpe (per-step, sign) — sample-level : -0.002436  (same CV style)\n",
      "Sharpe (per-step, sign) — minute-level : -0.004017  (portfolio level)\n",
      "Sharpe (per-step, long-only relative)  : 0.017159  (portfolio level)\n"
     ]
    }
   ],
   "source": [
    "# ===== 4) OOS evaluation metrics =====\n",
    "y_true_all = results_df_core[\"actual_log_return\"].values\n",
    "y_pred_all = results_df_core[\"predicted_log_return\"].values\n",
    "\n",
    "mse = mean_squared_error(y_true_all, y_pred_all)\n",
    "r2  = r2_score(y_true_all, y_pred_all)\n",
    "ic  = spearmanr(y_true_all, y_pred_all)[0]\n",
    "\n",
    "sr_step_sign_samples   = sharpe_per_step(results_df_core[\"pnl_sample_sign\"])         \n",
    "sr_step_sign_minutes   = sharpe_per_step(pnl_minute_sign)                        \n",
    "sr_step_relative_minutes = sharpe_per_step(pnl_minute_relative)                  \n",
    "\n",
    "print(\"\\n=== [OOS • Test Split] Performance ===\")\n",
    "print(f\"MSE                         : {mse:.8f}\")\n",
    "print(f\"R²                          : {r2:.6f}\")\n",
    "print(f\"IC (Spearman ρ)             : {ic:.6f}\")\n",
    "print(f\"Sharpe (per-step, sign) — sample-level : {sr_step_sign_samples:.6f}  (same CV style)\")\n",
    "print(f\"Sharpe (per-step, sign) — minute-level : {sr_step_sign_minutes:.6f}  (portfolio level)\")\n",
    "print(f\"Sharpe (per-step, long-only relative)  : {sr_step_relative_minutes:.6f}  (portfolio level)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8af6f4",
   "metadata": {},
   "source": [
    "### (Rolling Spearman ρ Stability Filtering)\n",
    "\n",
    "## Summary & Decision\n",
    "\n",
    "- **Performance:** Both out-of-sample IC and minute-level Sharpe ratios are lower than those of the main Lasso-selected feature model.\n",
    "- **Risk:** Drawdown levels show no meaningful improvement under comparable turnover conditions.\n",
    "- **Robustness:** After accounting for transaction costs, any marginal advantage disappears or turns negative.\n",
    "- **Interpretability:** Although the 5-feature set is simpler and more compact, its calibration is weaker and less monotonic.\n",
    "\n",
    "**Decision:**  \n",
    "This 5-feature variant is **not selected** for final portfolio construction after the rolling Spearman ρ stability screening.  \n",
    "It is retained here solely for transparency, reproducibility, and future reference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
