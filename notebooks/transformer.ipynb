{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "744ffb63",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sharpe(pnl_series, periods_per_year=390 * 252):\n",
    "    std = pnl_series.std()\n",
    "    if std == 0:\n",
    "        return 0.0\n",
    "    raw = pnl_series.mean() / std\n",
    "    return raw * np.sqrt(periods_per_year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6893de",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data/final_df.csv')\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Cannot find {DATA_PATH}\")\n",
    "\n",
    "df_ori = pd.read_csv(DATA_PATH)\n",
    "df = df_ori.copy()\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df = df.sort_values(['symbol', 'datetime']).reset_index(drop=True)\n",
    "\n",
    "df['lret_1m'] = df.groupby('symbol')['close'].transform(lambda s: np.log(s).diff())\n",
    "df['y_target'] = df.groupby('symbol')['lret_1m'].shift(-1)\n",
    "\n",
    "initial_rows = len(df)\n",
    "df = df[df['y_target'].abs() <= 0.2].dropna(subset=['y_target', 'lret_1m'])\n",
    "df = df.sort_values(['datetime', 'symbol']).reset_index(drop=True)\n",
    "print(f\"Dropped {initial_rows - len(df)} rows with abnormal/NaN targets\")\n",
    "print(df[['datetime', 'symbol', 'close', 'lret_1m', 'y_target']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf98e0",
   "metadata": {},
   "source": [
    "## Feature cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['y_target']\n",
    "X = df.drop(columns=[\n",
    "    'y_target', 'lret_1m', 'datetime', 'symbol',\n",
    "    'year', 'month', 'day', 'minute', 'minute_of_day'\n",
    "], errors='ignore')\n",
    "print(f\"Original feature shape: {X.shape}\")\n",
    "\n",
    "stats = X.describe(percentiles=[0.99]).T\n",
    "bad_cols = set()\n",
    "bad_cols.update(stats.index[stats['std'] > 1e3])\n",
    "bad_cols.update(stats.index[stats['99%'].abs() > 1e3])\n",
    "bad_cols.update(stats.index[stats['max'].abs() > 1e6])\n",
    "bad_cols.update(stats.index[stats['std'] == 0])\n",
    "\n",
    "bad_cols = sorted(bad_cols)\n",
    "print(f\"Dropping {len(bad_cols)} problematic columns\")\n",
    "X_cleaned = X.drop(columns=bad_cols)\n",
    "print(f\"Cleaned feature shape: {X_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c00fe9",
   "metadata": {},
   "source": [
    "## Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11810b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 1.0 / 1.5\n",
    "split_index = int(len(X_cleaned) * split_ratio)\n",
    "\n",
    "X_train = X_cleaned.iloc[:split_index]\n",
    "y_train = y.iloc[:split_index]\n",
    "X_test = X_cleaned.iloc[split_index:]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Test shape:  {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdc5abc",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c544e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileClipper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower=0.005, upper=0.995):\n",
    "        self.lower = lower\n",
    "        self.upper = upper\n",
    "        self.q_low_ = None\n",
    "        self.q_high_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.q_low_ = np.nanpercentile(X, self.lower * 100, axis=0)\n",
    "        self.q_high_ = np.nanpercentile(X, self.upper * 100, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.clip(X, self.q_low_, self.q_high_)\n",
    "\n",
    "feature_pipeline = Pipeline([\n",
    "    ('imp', SimpleImputer(strategy='median')),\n",
    "    ('clip', QuantileClipper(lower=0.005, upper=0.995)),\n",
    "    ('scale', MaxAbsScaler()),\n",
    "])\n",
    "\n",
    "feature_pipeline.fit(X_train)\n",
    "\n",
    "X_all_processed = pd.DataFrame(\n",
    "    feature_pipeline.transform(X_cleaned),\n",
    "    columns=X_cleaned.columns,\n",
    "    index=X_cleaned.index\n",
    ")\n",
    "\n",
    "print(\"Preprocessing complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af7bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = X_cleaned.columns.tolist()\n",
    "df_processed = df[['datetime', 'symbol', 'y_target']].copy()\n",
    "df_processed[feature_cols] = X_all_processed\n",
    "\n",
    "train_df = df_processed.iloc[:split_index].copy()\n",
    "test_df = df_processed.iloc[split_index:].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca2ebc6",
   "metadata": {},
   "source": [
    "## Sequence construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65032ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 60\n",
    "PRED_HORIZON = 1\n",
    "\n",
    "\n",
    "def build_sequences(dataframe, feature_cols, target_col, seq_len=SEQ_LEN):\n",
    "    X_seq, y_seq, meta_dt, meta_symbol = [], [], [], []\n",
    "    for symbol, group in dataframe.groupby('symbol'):\n",
    "        group = group.sort_values('datetime')\n",
    "        feats = group[feature_cols].values\n",
    "        target = group[target_col].values\n",
    "        dts = group['datetime'].values\n",
    "        for i in range(len(group) - seq_len - PRED_HORIZON + 1):\n",
    "            start = i\n",
    "            end = i + seq_len\n",
    "            target_idx = end + PRED_HORIZON - 1\n",
    "            X_seq.append(feats[start:end])\n",
    "            y_seq.append(target[target_idx])\n",
    "            meta_dt.append(dts[target_idx])\n",
    "            meta_symbol.append(symbol)\n",
    "    return (\n",
    "        np.array(X_seq, dtype=np.float32),\n",
    "        np.array(y_seq, dtype=np.float32),\n",
    "        np.array(meta_dt),\n",
    "        np.array(meta_symbol)\n",
    "    )\n",
    "\n",
    "train_X_seq, train_y_seq, train_meta_dt, train_meta_sym = build_sequences(\n",
    "    train_df, feature_cols, 'y_target', seq_len=SEQ_LEN\n",
    ")\n",
    "\n",
    "test_X_seq, test_y_seq, test_meta_dt, test_meta_sym = build_sequences(\n",
    "    test_df, feature_cols, 'y_target', seq_len=SEQ_LEN\n",
    ")\n",
    "\n",
    "print(f\"Train sequences: {train_X_seq.shape}\")\n",
    "print(f\"Test sequences:  {test_X_seq.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e55acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_order = np.argsort(train_meta_dt)\n",
    "train_X_seq = train_X_seq[train_order]\n",
    "train_y_seq = train_y_seq[train_order]\n",
    "train_meta_dt = train_meta_dt[train_order]\n",
    "\n",
    "test_order = np.argsort(test_meta_dt)\n",
    "test_X_seq = test_X_seq[test_order]\n",
    "test_y_seq = test_y_seq[test_order]\n",
    "test_meta_dt = test_meta_dt[test_order]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598dc5b",
   "metadata": {},
   "source": [
    "## Prefix K-fold DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e9382",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "def create_prefix_folds(X_seq, y_seq, n_splits=10, batch_size=BATCH_SIZE):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    folds = []\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X_seq), start=1):\n",
    "        X_tr, y_tr = X_seq[train_idx], y_seq[train_idx]\n",
    "        X_va, y_va = X_seq[val_idx], y_seq[val_idx]\n",
    "        train_ds = TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr))\n",
    "        val_ds = TensorDataset(torch.from_numpy(X_va), torch.from_numpy(y_va))\n",
    "        folds.append((\n",
    "            DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
    "            DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "        ))\n",
    "        print(f\"Fold {fold_idx}: train {len(train_ds)} seq, val {len(val_ds)} seq\")\n",
    "    return folds\n",
    "\n",
    "prefix_folds = create_prefix_folds(train_X_seq, train_y_seq, n_splits=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec4e1c4",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82760e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReturnTransformer(nn.Module):\n",
    "    def __init__(self, feature_dim, d_model=128, nhead=4, num_layers=2, ff_dim=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(feature_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        encoded = self.encoder(x)\n",
    "        return self.head(encoded[:, -1, :]).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e002c",
   "metadata": {},
   "source": [
    "## Training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e699d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(xb)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            total_loss += loss.item() * len(xb)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def predict_batches(model, X_array, batch_size=512):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, len(X_array), batch_size):\n",
    "            batch = torch.from_numpy(X_array[start:start + batch_size]).to(DEVICE)\n",
    "            preds.append(model(batch).cpu().numpy())\n",
    "    return np.concatenate(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d52e09",
   "metadata": {},
   "source": [
    "## Prefix CV training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e09f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def fit_with_prefix_cv(folds, feature_dim, epochs=15, lr=1e-3, weight_decay=1e-4, patience=3):\n",
    "    histories = []\n",
    "    best_states = []\n",
    "    for fold_idx, (train_loader, val_loader) in enumerate(folds, start=1):\n",
    "        model = ReturnTransformer(feature_dim).to(DEVICE)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.MSELoss()\n",
    "        best_val = math.inf\n",
    "        best_state = None\n",
    "        patience_ctr = 0\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "            val_loss = eval_epoch(model, val_loader, criterion)\n",
    "            if val_loss < best_val - 1e-6:\n",
    "                best_val = val_loss\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                patience_ctr = 0\n",
    "            else:\n",
    "                patience_ctr += 1\n",
    "            print(f\"Fold {fold_idx} | Epoch {epoch} | train {train_loss:.6f} | val {val_loss:.6f}\")\n",
    "            if patience_ctr >= patience:\n",
    "                print(f\"Stopping early on fold {fold_idx}\")\n",
    "                break\n",
    "        histories.append({'fold': fold_idx, 'best_val_mse': best_val})\n",
    "        best_states.append(best_state)\n",
    "    return histories, best_states\n",
    "\n",
    "cv_histories, cv_states = fit_with_prefix_cv(\n",
    "    prefix_folds,\n",
    "    feature_dim=len(feature_cols),\n",
    "    epochs=12,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    patience=3,\n",
    ")\n",
    "cv_histories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36e35d",
   "metadata": {},
   "source": [
    "## Final training on full training window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SPLIT = int(len(train_X_seq) * 0.9)\n",
    "full_train_ds = TensorDataset(\n",
    "    torch.from_numpy(train_X_seq[:VALID_SPLIT]),\n",
    "    torch.from_numpy(train_y_seq[:VALID_SPLIT])\n",
    ")\n",
    "full_val_ds = TensorDataset(\n",
    "    torch.from_numpy(train_X_seq[VALID_SPLIT:]),\n",
    "    torch.from_numpy(train_y_seq[VALID_SPLIT:])\n",
    ")\n",
    "full_train_loader = DataLoader(full_train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "full_val_loader = DataLoader(full_val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "final_model = ReturnTransformer(len(feature_cols)).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(final_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "best_state = None\n",
    "best_val = math.inf\n",
    "for epoch in range(1, 21):\n",
    "    train_loss = train_epoch(final_model, full_train_loader, optimizer, criterion)\n",
    "    val_loss = eval_epoch(final_model, full_val_loader, criterion)\n",
    "    print(f\"Epoch {epoch} | train {train_loss:.6f} | val {val_loss:.6f}\")\n",
    "    if val_loss < best_val - 1e-6:\n",
    "        best_val = val_loss\n",
    "        best_state = copy.deepcopy(final_model.state_dict())\n",
    "final_model.load_state_dict(best_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e69b96e",
   "metadata": {},
   "source": [
    "## Sharpe evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57299cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = predict_batches(final_model, train_X_seq)\n",
    "train_pnl = pd.Series(train_preds * train_y_seq)\n",
    "train_sharpe = calculate_sharpe(train_pnl)\n",
    "\n",
    "test_preds = predict_batches(final_model, test_X_seq)\n",
    "test_pnl = pd.Series(test_preds * test_y_seq)\n",
    "test_sharpe = calculate_sharpe(test_pnl)\n",
    "\n",
    "print(f\"Train Sharpe: {train_sharpe:.6f}\")\n",
    "print(f\"Test Sharpe:  {test_sharpe:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6297dba",
   "metadata": {},
   "source": [
    "## Rolling 30-minute backtest (optional heavy cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLLING_WINDOW_MINUTES = 30\n",
    "MIN_SEQ_PER_WINDOW = 50\n",
    "\n",
    "def build_window_sequences(window_df, feature_cols, seq_len=SEQ_LEN):\n",
    "    X_seq, y_seq = [], []\n",
    "    for symbol, group in window_df.groupby('symbol'):\n",
    "        group = group.sort_values('datetime')\n",
    "        if len(group) < seq_len + 1:\n",
    "            continue\n",
    "        feats = group[feature_cols].values\n",
    "        target = group['y_target'].values\n",
    "        for i in range(len(group) - seq_len):\n",
    "            X_seq.append(feats[i:i+seq_len])\n",
    "            y_seq.append(target[i+seq_len])\n",
    "    if not X_seq:\n",
    "        return None, None\n",
    "    return np.array(X_seq, dtype=np.float32), np.array(y_seq, dtype=np.float32)\n",
    "\n",
    "backtest_results = []\n",
    "unique_test_minutes = np.sort(test_df['datetime'].unique())\n",
    "\n",
    "for current_dt in tqdm(unique_test_minutes, desc='Rolling backtest'):\n",
    "    start_dt = current_dt - pd.Timedelta(minutes=ROLLING_WINDOW_MINUTES)\n",
    "    window_df = df_processed[(df_processed['datetime'] >= start_dt) & (df_processed['datetime'] < current_dt)]\n",
    "    X_tr, y_tr = build_window_sequences(window_df, feature_cols, seq_len=SEQ_LEN)\n",
    "    if X_tr is None or len(X_tr) < MIN_SEQ_PER_WINDOW:\n",
    "        continue\n",
    "    train_loader = DataLoader(TensorDataset(torch.from_numpy(X_tr), torch.from_numpy(y_tr)), batch_size=128, shuffle=True)\n",
    "    temp_model = ReturnTransformer(len(feature_cols)).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(temp_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    for _ in range(3):\n",
    "        train_epoch(temp_model, train_loader, optimizer, criterion)\n",
    "\n",
    "    # Build prediction sequences ending at current_dt for each symbol\n",
    "    pred_rows = df_processed[df_processed['datetime'] == current_dt]\n",
    "    symbol_preds = []\n",
    "    combined = pd.concat([window_df, pred_rows], ignore_index=True)\n",
    "    for symbol, group in combined.groupby('symbol'):\n",
    "        group = group.sort_values('datetime')\n",
    "        if group['datetime'].iloc[-1] != current_dt:\n",
    "            continue\n",
    "        if len(group) < SEQ_LEN:\n",
    "            continue\n",
    "        seq = group[feature_cols].values[-SEQ_LEN:]\n",
    "        symbol_preds.append((symbol, seq))\n",
    "\n",
    "    if not symbol_preds:\n",
    "        continue\n",
    "\n",
    "    batch = torch.from_numpy(np.stack([seq for _, seq in symbol_preds])).to(DEVICE)\n",
    "    temp_model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = temp_model(batch).cpu().numpy()\n",
    "\n",
    "    actuals = df_processed[df_processed['datetime'] == current_dt][['symbol', 'y_target']]\n",
    "    actual_map = dict(zip(actuals['symbol'], actuals['y_target']))\n",
    "    for (symbol, _), pred in zip(symbol_preds, preds):\n",
    "        if symbol not in actual_map:\n",
    "            continue\n",
    "        backtest_results.append({\n",
    "            'datetime': current_dt,\n",
    "            'symbol': symbol,\n",
    "            'predicted_log_return': pred,\n",
    "            'actual_log_return': actual_map[symbol]\n",
    "        })\n",
    "\n",
    "backtest_df = pd.DataFrame(backtest_results)\n",
    "if not backtest_df.empty:\n",
    "    backtest_df['positive_prediction'] = backtest_df['predicted_log_return'].clip(lower=0)\n",
    "    minute_sum = backtest_df.groupby('datetime')['positive_prediction'].transform('sum')\n",
    "    backtest_df['weight_relative'] = np.where(minute_sum == 0, 0.0, backtest_df['positive_prediction'] / minute_sum)\n",
    "    backtest_df.drop(columns=['positive_prediction'], inplace=True)\n",
    "    backtest_df['weight_sign'] = np.sign(backtest_df['predicted_log_return'])\n",
    "    print(backtest_df.head())\n",
    "    print(f\"Backtest rows: {len(backtest_df)}\")\n",
    "else:\n",
    "    print(\"Backtest skipped due to insufficient sequences\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa5205-p1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
